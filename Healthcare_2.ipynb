{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeEzeWWnHV8Rec01RqCP4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanthoshiRavi/Healthcare-Text-Processing-/blob/main/Healthcare_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgWgTg8UsWR1",
        "outputId": "86dfba49-5fea-42db-8dad-b93481f5f796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7W08WCvsxBC",
        "outputId": "4f9aebbe-a963-4c75-d80f-c7ad78668586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWbLJX9ts7FO",
        "outputId": "d80e09ff-1607-43ad-b89f-baff7f073431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def fetch_pubmed_article_ids(query, api_key, max_results=100):\n",
        "    url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={query}&retmax={max_results}&api_key={api_key}&retmode=json'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        ids = data['esearchresult']['idlist']\n",
        "        return ids\n",
        "    else:\n",
        "        print(f\"Failed to fetch data: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Your API key\n",
        "api_key = '14b3330cab5a4aabaefc418267dd19492909'\n",
        "# Query term\n",
        "query = 'diabetes treatment'\n",
        "# Fetch article IDs\n",
        "article_ids = fetch_pubmed_article_ids(query, api_key)\n",
        "print(\"Article IDs fetched:\")\n",
        "print(article_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFGvffCItAKb",
        "outputId": "8de39d35-f8a9-4598-fab6-76b1845fa37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article IDs fetched:\n",
            "['38760852', '38760705', '38760678', '38760661', '38760632', '38760595', '38760456', '38760452', '38760427', '38760383', '38760126', '38760125', '38760053', '38760035', '38760033', '38759989', '38759983', '38759874', '38759763', '38759722', '38759707', '38759658', '38759515', '38759430', '38759418', '38759318', '38759315', '38759226', '38759099', '38758937', '38758936', '38758907', '38758866', '38758863', '38758855', '38758848', '38758847', '38758689', '38758687', '38758679', '38758678', '38758675', '38758674', '38758639', '38758517', '38758435', '38758423', '38758248', '38758212', '38758211', '38758187', '38758154', '38758099', '38757999', '38757954', '38757950', '38757904', '38757902', '38757785', '38757729', '38757725', '38757633', '38757582', '38757550', '38757431', '38757421', '38757360', '38757342', '38757316', '38757277', '38757211', '38757202', '38757185', '38757094', '38757051', '38756658', '38756527', '38756477', '38756429', '38756426', '38756391', '38756268', '38756263', '38756229', '38756206', '38756167', '38756144', '38756107', '38756105', '38756050', '38756007', '38755956', '38755721', '38755706', '38755644', '38755631', '38755621', '38755602', '38755559', '38755503']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_pubmed_article_details(article_ids, api_key):\n",
        "    ids = ','.join(article_ids)\n",
        "    url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={ids}&api_key={api_key}&retmode=xml'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch data: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "article_details_xml = fetch_pubmed_article_details(article_ids, api_key)\n",
        "print(\"Fetched article details in XML format.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0yoazCdtOPM",
        "outputId": "273223a9-77b2-447f-9001-5fc04997739c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched article details in XML format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def parse_article_details(article_xml):\n",
        "    soup = BeautifulSoup(article_xml, 'xml')\n",
        "    articles = soup.find_all('PubmedArticle')\n",
        "    parsed_data = []\n",
        "    for article in articles:\n",
        "        title = article.find('ArticleTitle').text if article.find('ArticleTitle') else \"\"\n",
        "        abstract = article.find('AbstractText').text if article.find('AbstractText') else \"\"\n",
        "        parsed_data.append((title, abstract))\n",
        "    return parsed_data\n",
        "\n",
        "parsed_articles = parse_article_details(article_details_xml)\n",
        "print(\"Parsed articles:\")\n",
        "print(parsed_articles[:3])  # Print the first 3 articles for verification\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "preprocessed_articles = [(preprocess_text(title), preprocess_text(abstract)) for title, abstract in parsed_articles]\n",
        "print(\"Preprocessed articles:\")\n",
        "print(preprocessed_articles[:3])  # Print the first 3 preprocessed articles for verification\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfbEt0yJtSTI",
        "outputId": "b93018d6-7575-4006-bfb9-9ac0ed0c023e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed articles:\n",
            "[('Response to semaglutide of non-drinker subjects with type 2 diabetes.', 'Metabolic dysfunction-associated Steatotic Liver Disease (MASLD) displays a worse prognosis in subjects with type 2 diabetes (T2D); effective treatments are, so far, scanty. Semaglutide showed efficacy in improving steatohepatitis. We longitudinally observed a MASLD cohort of T2D subjects starting semaglutide, to detect an improvement of non-invasive surrogates of steatosis and fibro-inflammatory liver involvement, evaluating the role of mild alcohol consumption.'), ('Characterization of the gut microbiota in polycystic ovary syndrome with dyslipidemia.', 'Polycystic ovary syndrome (PCOS) is an endocrinopathy in childbearing-age females which can cause many complications, such as diabetes, obesity, and dyslipidemia. The metabolic disorders in patients with PCOS were linked to gut microbial dysbiosis. However, the correlation between the gut microbial community and dyslipidemia in PCOS remains unillustrated. Our study elucidated the different gut microbiota in patients with PCOS and dyslipidemia (PCOS.D) compared to those with only PCOS and healthy women.'), ('AMPK activation eliminates senescent cells in diabetic wound by inducing NCOA4 mediated ferritinophagy.', 'Diabetic wounds are one of the long-term complications of diabetes, with a disordered microenvironment, diabetic wounds can easily develop into chronic non-healing wounds, which can impose a significant burden on healthcare. In diabetic condition, senescent cells accumulate in the wound area and suppress the wound healing process. AMPK, as a molecule related to metabolism, has a close relationship with aging and diabetes. The purpose of this study was to investigate the effects of AMPK activation on wound healing and explore the underlying mechanisms.')]\n",
            "Preprocessed articles:\n",
            "[('Response semaglutide nondrinker subject type diabetes', 'Metabolic dysfunctionassociated Steatotic Liver Disease MASLD display worse prognosis subject type diabetes TD effective treatment far scanty Semaglutide showed efficacy improving steatohepatitis longitudinally observed MASLD cohort TD subject starting semaglutide detect improvement noninvasive surrogate steatosis fibroinflammatory liver involvement evaluating role mild alcohol consumption'), ('Characterization gut microbiota polycystic ovary syndrome dyslipidemia', 'Polycystic ovary syndrome PCOS endocrinopathy childbearingage female cause many complication diabetes obesity dyslipidemia metabolic disorder patient PCOS linked gut microbial dysbiosis However correlation gut microbial community dyslipidemia PCOS remains unillustrated study elucidated different gut microbiota patient PCOS dyslipidemia PCOSD compared PCOS healthy woman'), ('AMPK activation eliminates senescent cell diabetic wound inducing NCOA mediated ferritinophagy', 'Diabetic wound one longterm complication diabetes disordered microenvironment diabetic wound easily develop chronic nonhealing wound impose significant burden healthcare diabetic condition senescent cell accumulate wound area suppress wound healing process AMPK molecule related metabolism close relationship aging diabetes purpose study investigate effect AMPK activation wound healing explore underlying mechanism')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Combine title and abstract for each article\n",
        "combined_texts = [f\"{title} {abstract}\" for title, abstract in preprocessed_articles]\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Fit and transform the combined texts\n",
        "X = vectorizer.fit_transform(combined_texts)\n",
        "\n",
        "# Print the shape of the TF-IDF matrix\n",
        "print(f\"TF-IDF matrix shape: {X.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLhh66jctuGW",
        "outputId": "72860e12-5cd7-4d59-adfb-5bde73f1781e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix shape: (100, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Number of topics\n",
        "n_topics = 5\n",
        "\n",
        "# Fit LDA model\n",
        "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "lda_topics = lda_model.fit_transform(X)\n",
        "\n",
        "# Print the topics and the top words in each topic\n",
        "print(\"Topics found via LDA:\")\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for idx, topic in enumerate(lda_model.components_):\n",
        "    print(f\"Topic {idx + 1}:\")\n",
        "    print([feature_names[i] for i in topic.argsort()[-10:]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3myz3C1twlw",
        "outputId": "7b9a4a82-13fc-40ed-aab5-00720398e8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics found via LDA:\n",
            "Topic 1:\n",
            "['brain', 'subject', 'syndrome', 'insulin', 'drug', 'hfpef', 'diabetes', 'study', 'semaglutide', 'patient']\n",
            "Topic 2:\n",
            "['patient', 'study', 'amputation', 'diabetes', 'disease', 'foot', 'clinical', 'group', 'treatment', 'diabetic']\n",
            "Topic 3:\n",
            "['tdm', 'extract', 'methylation', 'orion', 'lipodystrophy', 'dress', 'diabetic', 'ulceration', 'metabolic', 'foot']\n",
            "Topic 4:\n",
            "['diabetes', 'treatment', 'diabetic', 'wound', 'cell', 'effect', 'disease', 'risk', 'kidney', 'patient']\n",
            "Topic 5:\n",
            "['opportunity', 'wound', 'type', 'group', 'challenge', 'level', 'health', 'covid', 'diabetes', 'patient']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Print the topic distribution for each document\n",
        "for idx, topic_distribution in enumerate(lda_topics):\n",
        "    print(f\"Document {idx + 1}:\")\n",
        "    for topic_idx, prob in enumerate(topic_distribution):\n",
        "        print(f\"  Topic {topic_idx + 1}: {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6PYi6z_vKt2",
        "outputId": "29f67d1f-b4b8-49c8-b688-54b3ce054390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "  Topic 1: 0.8285\n",
            "  Topic 2: 0.0428\n",
            "  Topic 3: 0.0427\n",
            "  Topic 4: 0.0427\n",
            "  Topic 5: 0.0433\n",
            "Document 2:\n",
            "  Topic 1: 0.8287\n",
            "  Topic 2: 0.0430\n",
            "  Topic 3: 0.0426\n",
            "  Topic 4: 0.0427\n",
            "  Topic 5: 0.0429\n",
            "Document 3:\n",
            "  Topic 1: 0.0377\n",
            "  Topic 2: 0.0380\n",
            "  Topic 3: 0.0377\n",
            "  Topic 4: 0.8486\n",
            "  Topic 5: 0.0380\n",
            "Document 4:\n",
            "  Topic 1: 0.0404\n",
            "  Topic 2: 0.0402\n",
            "  Topic 3: 0.0404\n",
            "  Topic 4: 0.0402\n",
            "  Topic 5: 0.8388\n",
            "Document 5:\n",
            "  Topic 1: 0.0352\n",
            "  Topic 2: 0.8614\n",
            "  Topic 3: 0.0345\n",
            "  Topic 4: 0.0345\n",
            "  Topic 5: 0.0344\n",
            "Document 6:\n",
            "  Topic 1: 0.0412\n",
            "  Topic 2: 0.8358\n",
            "  Topic 3: 0.0409\n",
            "  Topic 4: 0.0410\n",
            "  Topic 5: 0.0411\n",
            "Document 7:\n",
            "  Topic 1: 0.8782\n",
            "  Topic 2: 0.0306\n",
            "  Topic 3: 0.0303\n",
            "  Topic 4: 0.0303\n",
            "  Topic 5: 0.0304\n",
            "Document 8:\n",
            "  Topic 1: 0.0285\n",
            "  Topic 2: 0.0276\n",
            "  Topic 3: 0.8887\n",
            "  Topic 4: 0.0275\n",
            "  Topic 5: 0.0276\n",
            "Document 9:\n",
            "  Topic 1: 0.0350\n",
            "  Topic 2: 0.0351\n",
            "  Topic 3: 0.0346\n",
            "  Topic 4: 0.8599\n",
            "  Topic 5: 0.0355\n",
            "Document 10:\n",
            "  Topic 1: 0.0365\n",
            "  Topic 2: 0.8566\n",
            "  Topic 3: 0.0353\n",
            "  Topic 4: 0.0358\n",
            "  Topic 5: 0.0358\n",
            "Document 11:\n",
            "  Topic 1: 0.0311\n",
            "  Topic 2: 0.0311\n",
            "  Topic 3: 0.0306\n",
            "  Topic 4: 0.0309\n",
            "  Topic 5: 0.8762\n",
            "Document 12:\n",
            "  Topic 1: 0.0295\n",
            "  Topic 2: 0.0292\n",
            "  Topic 3: 0.0294\n",
            "  Topic 4: 0.0293\n",
            "  Topic 5: 0.8825\n",
            "Document 13:\n",
            "  Topic 1: 0.0363\n",
            "  Topic 2: 0.0361\n",
            "  Topic 3: 0.0360\n",
            "  Topic 4: 0.0361\n",
            "  Topic 5: 0.8555\n",
            "Document 14:\n",
            "  Topic 1: 0.8219\n",
            "  Topic 2: 0.0447\n",
            "  Topic 3: 0.0444\n",
            "  Topic 4: 0.0446\n",
            "  Topic 5: 0.0445\n",
            "Document 15:\n",
            "  Topic 1: 0.0334\n",
            "  Topic 2: 0.0329\n",
            "  Topic 3: 0.0335\n",
            "  Topic 4: 0.0333\n",
            "  Topic 5: 0.8669\n",
            "Document 16:\n",
            "  Topic 1: 0.0531\n",
            "  Topic 2: 0.0530\n",
            "  Topic 3: 0.0529\n",
            "  Topic 4: 0.0529\n",
            "  Topic 5: 0.7882\n",
            "Document 17:\n",
            "  Topic 1: 0.8526\n",
            "  Topic 2: 0.0372\n",
            "  Topic 3: 0.0366\n",
            "  Topic 4: 0.0369\n",
            "  Topic 5: 0.0368\n",
            "Document 18:\n",
            "  Topic 1: 0.0304\n",
            "  Topic 2: 0.0304\n",
            "  Topic 3: 0.0300\n",
            "  Topic 4: 0.8787\n",
            "  Topic 5: 0.0306\n",
            "Document 19:\n",
            "  Topic 1: 0.0354\n",
            "  Topic 2: 0.0349\n",
            "  Topic 3: 0.8599\n",
            "  Topic 4: 0.0348\n",
            "  Topic 5: 0.0349\n",
            "Document 20:\n",
            "  Topic 1: 0.0705\n",
            "  Topic 2: 0.0705\n",
            "  Topic 3: 0.0703\n",
            "  Topic 4: 0.0706\n",
            "  Topic 5: 0.7180\n",
            "Document 21:\n",
            "  Topic 1: 0.8797\n",
            "  Topic 2: 0.0298\n",
            "  Topic 3: 0.0299\n",
            "  Topic 4: 0.0305\n",
            "  Topic 5: 0.0301\n",
            "Document 22:\n",
            "  Topic 1: 0.0357\n",
            "  Topic 2: 0.8575\n",
            "  Topic 3: 0.0354\n",
            "  Topic 4: 0.0358\n",
            "  Topic 5: 0.0355\n",
            "Document 23:\n",
            "  Topic 1: 0.0321\n",
            "  Topic 2: 0.0318\n",
            "  Topic 3: 0.0314\n",
            "  Topic 4: 0.0322\n",
            "  Topic 5: 0.8725\n",
            "Document 24:\n",
            "  Topic 1: 0.8751\n",
            "  Topic 2: 0.0314\n",
            "  Topic 3: 0.0310\n",
            "  Topic 4: 0.0314\n",
            "  Topic 5: 0.0310\n",
            "Document 25:\n",
            "  Topic 1: 0.8572\n",
            "  Topic 2: 0.0356\n",
            "  Topic 3: 0.0354\n",
            "  Topic 4: 0.0362\n",
            "  Topic 5: 0.0356\n",
            "Document 26:\n",
            "  Topic 1: 0.0384\n",
            "  Topic 2: 0.8461\n",
            "  Topic 3: 0.0387\n",
            "  Topic 4: 0.0384\n",
            "  Topic 5: 0.0384\n",
            "Document 27:\n",
            "  Topic 1: 0.0452\n",
            "  Topic 2: 0.0461\n",
            "  Topic 3: 0.0456\n",
            "  Topic 4: 0.8179\n",
            "  Topic 5: 0.0451\n",
            "Document 28:\n",
            "  Topic 1: 0.0311\n",
            "  Topic 2: 0.8770\n",
            "  Topic 3: 0.0301\n",
            "  Topic 4: 0.0308\n",
            "  Topic 5: 0.0310\n",
            "Document 29:\n",
            "  Topic 1: 0.0363\n",
            "  Topic 2: 0.8551\n",
            "  Topic 3: 0.0361\n",
            "  Topic 4: 0.0363\n",
            "  Topic 5: 0.0362\n",
            "Document 30:\n",
            "  Topic 1: 0.0368\n",
            "  Topic 2: 0.0368\n",
            "  Topic 3: 0.8528\n",
            "  Topic 4: 0.0367\n",
            "  Topic 5: 0.0369\n",
            "Document 31:\n",
            "  Topic 1: 0.8927\n",
            "  Topic 2: 0.0269\n",
            "  Topic 3: 0.0271\n",
            "  Topic 4: 0.0265\n",
            "  Topic 5: 0.0268\n",
            "Document 32:\n",
            "  Topic 1: 0.0332\n",
            "  Topic 2: 0.0338\n",
            "  Topic 3: 0.0331\n",
            "  Topic 4: 0.0330\n",
            "  Topic 5: 0.8669\n",
            "Document 33:\n",
            "  Topic 1: 0.0319\n",
            "  Topic 2: 0.0326\n",
            "  Topic 3: 0.0315\n",
            "  Topic 4: 0.0318\n",
            "  Topic 5: 0.8721\n",
            "Document 34:\n",
            "  Topic 1: 0.0392\n",
            "  Topic 2: 0.0396\n",
            "  Topic 3: 0.0397\n",
            "  Topic 4: 0.8422\n",
            "  Topic 5: 0.0393\n",
            "Document 35:\n",
            "  Topic 1: 0.8948\n",
            "  Topic 2: 0.0265\n",
            "  Topic 3: 0.0262\n",
            "  Topic 4: 0.0262\n",
            "  Topic 5: 0.0263\n",
            "Document 36:\n",
            "  Topic 1: 0.0361\n",
            "  Topic 2: 0.8553\n",
            "  Topic 3: 0.0364\n",
            "  Topic 4: 0.0361\n",
            "  Topic 5: 0.0361\n",
            "Document 37:\n",
            "  Topic 1: 0.8917\n",
            "  Topic 2: 0.0272\n",
            "  Topic 3: 0.0270\n",
            "  Topic 4: 0.0270\n",
            "  Topic 5: 0.0271\n",
            "Document 38:\n",
            "  Topic 1: 0.8241\n",
            "  Topic 2: 0.0445\n",
            "  Topic 3: 0.0437\n",
            "  Topic 4: 0.0440\n",
            "  Topic 5: 0.0438\n",
            "Document 39:\n",
            "  Topic 1: 0.0504\n",
            "  Topic 2: 0.0514\n",
            "  Topic 3: 0.7966\n",
            "  Topic 4: 0.0509\n",
            "  Topic 5: 0.0507\n",
            "Document 40:\n",
            "  Topic 1: 0.0325\n",
            "  Topic 2: 0.8702\n",
            "  Topic 3: 0.0323\n",
            "  Topic 4: 0.0325\n",
            "  Topic 5: 0.0324\n",
            "Document 41:\n",
            "  Topic 1: 0.8183\n",
            "  Topic 2: 0.0461\n",
            "  Topic 3: 0.0452\n",
            "  Topic 4: 0.0452\n",
            "  Topic 5: 0.0452\n",
            "Document 42:\n",
            "  Topic 1: 0.0566\n",
            "  Topic 2: 0.7764\n",
            "  Topic 3: 0.0555\n",
            "  Topic 4: 0.0555\n",
            "  Topic 5: 0.0559\n",
            "Document 43:\n",
            "  Topic 1: 0.0369\n",
            "  Topic 2: 0.8525\n",
            "  Topic 3: 0.0368\n",
            "  Topic 4: 0.0369\n",
            "  Topic 5: 0.0370\n",
            "Document 44:\n",
            "  Topic 1: 0.0459\n",
            "  Topic 2: 0.8164\n",
            "  Topic 3: 0.0458\n",
            "  Topic 4: 0.0461\n",
            "  Topic 5: 0.0459\n",
            "Document 45:\n",
            "  Topic 1: 0.8684\n",
            "  Topic 2: 0.0331\n",
            "  Topic 3: 0.0328\n",
            "  Topic 4: 0.0328\n",
            "  Topic 5: 0.0329\n",
            "Document 46:\n",
            "  Topic 1: 0.0294\n",
            "  Topic 2: 0.0286\n",
            "  Topic 3: 0.0287\n",
            "  Topic 4: 0.8846\n",
            "  Topic 5: 0.0288\n",
            "Document 47:\n",
            "  Topic 1: 0.8166\n",
            "  Topic 2: 0.0457\n",
            "  Topic 3: 0.0456\n",
            "  Topic 4: 0.0459\n",
            "  Topic 5: 0.0463\n",
            "Document 48:\n",
            "  Topic 1: 0.0349\n",
            "  Topic 2: 0.0358\n",
            "  Topic 3: 0.0343\n",
            "  Topic 4: 0.0349\n",
            "  Topic 5: 0.8602\n",
            "Document 49:\n",
            "  Topic 1: 0.0377\n",
            "  Topic 2: 0.0375\n",
            "  Topic 3: 0.0372\n",
            "  Topic 4: 0.0372\n",
            "  Topic 5: 0.8504\n",
            "Document 50:\n",
            "  Topic 1: 0.0264\n",
            "  Topic 2: 0.0263\n",
            "  Topic 3: 0.0258\n",
            "  Topic 4: 0.8952\n",
            "  Topic 5: 0.0263\n",
            "Document 51:\n",
            "  Topic 1: 0.0250\n",
            "  Topic 2: 0.0253\n",
            "  Topic 3: 0.0251\n",
            "  Topic 4: 0.0250\n",
            "  Topic 5: 0.8995\n",
            "Document 52:\n",
            "  Topic 1: 0.0474\n",
            "  Topic 2: 0.0473\n",
            "  Topic 3: 0.0469\n",
            "  Topic 4: 0.8114\n",
            "  Topic 5: 0.0470\n",
            "Document 53:\n",
            "  Topic 1: 0.0368\n",
            "  Topic 2: 0.0370\n",
            "  Topic 3: 0.0362\n",
            "  Topic 4: 0.8537\n",
            "  Topic 5: 0.0363\n",
            "Document 54:\n",
            "  Topic 1: 0.0398\n",
            "  Topic 2: 0.8398\n",
            "  Topic 3: 0.0402\n",
            "  Topic 4: 0.0402\n",
            "  Topic 5: 0.0400\n",
            "Document 55:\n",
            "  Topic 1: 0.8956\n",
            "  Topic 2: 0.0260\n",
            "  Topic 3: 0.0260\n",
            "  Topic 4: 0.0260\n",
            "  Topic 5: 0.0263\n",
            "Document 56:\n",
            "  Topic 1: 0.0540\n",
            "  Topic 2: 0.0541\n",
            "  Topic 3: 0.7841\n",
            "  Topic 4: 0.0539\n",
            "  Topic 5: 0.0538\n",
            "Document 57:\n",
            "  Topic 1: 0.0372\n",
            "  Topic 2: 0.0369\n",
            "  Topic 3: 0.0366\n",
            "  Topic 4: 0.8521\n",
            "  Topic 5: 0.0372\n",
            "Document 58:\n",
            "  Topic 1: 0.8582\n",
            "  Topic 2: 0.0355\n",
            "  Topic 3: 0.0352\n",
            "  Topic 4: 0.0358\n",
            "  Topic 5: 0.0354\n",
            "Document 59:\n",
            "  Topic 1: 0.0375\n",
            "  Topic 2: 0.8513\n",
            "  Topic 3: 0.0367\n",
            "  Topic 4: 0.0376\n",
            "  Topic 5: 0.0368\n",
            "Document 60:\n",
            "  Topic 1: 0.0517\n",
            "  Topic 2: 0.0516\n",
            "  Topic 3: 0.0515\n",
            "  Topic 4: 0.0523\n",
            "  Topic 5: 0.7929\n",
            "Document 61:\n",
            "  Topic 1: 0.0447\n",
            "  Topic 2: 0.0442\n",
            "  Topic 3: 0.8224\n",
            "  Topic 4: 0.0441\n",
            "  Topic 5: 0.0446\n",
            "Document 62:\n",
            "  Topic 1: 0.0327\n",
            "  Topic 2: 0.0326\n",
            "  Topic 3: 0.0323\n",
            "  Topic 4: 0.0324\n",
            "  Topic 5: 0.8700\n",
            "Document 63:\n",
            "  Topic 1: 0.0382\n",
            "  Topic 2: 0.8481\n",
            "  Topic 3: 0.0381\n",
            "  Topic 4: 0.0378\n",
            "  Topic 5: 0.0378\n",
            "Document 64:\n",
            "  Topic 1: 0.8833\n",
            "  Topic 2: 0.0296\n",
            "  Topic 3: 0.0288\n",
            "  Topic 4: 0.0291\n",
            "  Topic 5: 0.0291\n",
            "Document 65:\n",
            "  Topic 1: 0.8542\n",
            "  Topic 2: 0.0364\n",
            "  Topic 3: 0.0364\n",
            "  Topic 4: 0.0365\n",
            "  Topic 5: 0.0365\n",
            "Document 66:\n",
            "  Topic 1: 0.0391\n",
            "  Topic 2: 0.0394\n",
            "  Topic 3: 0.8435\n",
            "  Topic 4: 0.0389\n",
            "  Topic 5: 0.0391\n",
            "Document 67:\n",
            "  Topic 1: 0.0317\n",
            "  Topic 2: 0.0311\n",
            "  Topic 3: 0.0311\n",
            "  Topic 4: 0.8750\n",
            "  Topic 5: 0.0312\n",
            "Document 68:\n",
            "  Topic 1: 0.0275\n",
            "  Topic 2: 0.0269\n",
            "  Topic 3: 0.0270\n",
            "  Topic 4: 0.0271\n",
            "  Topic 5: 0.8915\n",
            "Document 69:\n",
            "  Topic 1: 0.0362\n",
            "  Topic 2: 0.0364\n",
            "  Topic 3: 0.0363\n",
            "  Topic 4: 0.8549\n",
            "  Topic 5: 0.0362\n",
            "Document 70:\n",
            "  Topic 1: 0.0398\n",
            "  Topic 2: 0.8408\n",
            "  Topic 3: 0.0397\n",
            "  Topic 4: 0.0400\n",
            "  Topic 5: 0.0397\n",
            "Document 71:\n",
            "  Topic 1: 0.0328\n",
            "  Topic 2: 0.8694\n",
            "  Topic 3: 0.0325\n",
            "  Topic 4: 0.0326\n",
            "  Topic 5: 0.0327\n",
            "Document 72:\n",
            "  Topic 1: 0.0738\n",
            "  Topic 2: 0.7041\n",
            "  Topic 3: 0.0735\n",
            "  Topic 4: 0.0741\n",
            "  Topic 5: 0.0745\n",
            "Document 73:\n",
            "  Topic 1: 0.8930\n",
            "  Topic 2: 0.0269\n",
            "  Topic 3: 0.0266\n",
            "  Topic 4: 0.0267\n",
            "  Topic 5: 0.0269\n",
            "Document 74:\n",
            "  Topic 1: 0.0306\n",
            "  Topic 2: 0.0299\n",
            "  Topic 3: 0.0299\n",
            "  Topic 4: 0.0302\n",
            "  Topic 5: 0.8794\n",
            "Document 75:\n",
            "  Topic 1: 0.0402\n",
            "  Topic 2: 0.0401\n",
            "  Topic 3: 0.0396\n",
            "  Topic 4: 0.8404\n",
            "  Topic 5: 0.0398\n",
            "Document 76:\n",
            "  Topic 1: 0.0428\n",
            "  Topic 2: 0.0420\n",
            "  Topic 3: 0.0419\n",
            "  Topic 4: 0.8314\n",
            "  Topic 5: 0.0419\n",
            "Document 77:\n",
            "  Topic 1: 0.0390\n",
            "  Topic 2: 0.0398\n",
            "  Topic 3: 0.0392\n",
            "  Topic 4: 0.8432\n",
            "  Topic 5: 0.0387\n",
            "Document 78:\n",
            "  Topic 1: 0.8696\n",
            "  Topic 2: 0.0330\n",
            "  Topic 3: 0.0322\n",
            "  Topic 4: 0.0324\n",
            "  Topic 5: 0.0328\n",
            "Document 79:\n",
            "  Topic 1: 0.8522\n",
            "  Topic 2: 0.0371\n",
            "  Topic 3: 0.0369\n",
            "  Topic 4: 0.0369\n",
            "  Topic 5: 0.0368\n",
            "Document 80:\n",
            "  Topic 1: 0.8207\n",
            "  Topic 2: 0.0450\n",
            "  Topic 3: 0.0447\n",
            "  Topic 4: 0.0450\n",
            "  Topic 5: 0.0446\n",
            "Document 81:\n",
            "  Topic 1: 0.0504\n",
            "  Topic 2: 0.0504\n",
            "  Topic 3: 0.0503\n",
            "  Topic 4: 0.0503\n",
            "  Topic 5: 0.7986\n",
            "Document 82:\n",
            "  Topic 1: 0.0308\n",
            "  Topic 2: 0.8777\n",
            "  Topic 3: 0.0302\n",
            "  Topic 4: 0.0306\n",
            "  Topic 5: 0.0307\n",
            "Document 83:\n",
            "  Topic 1: 0.0243\n",
            "  Topic 2: 0.9036\n",
            "  Topic 3: 0.0239\n",
            "  Topic 4: 0.0241\n",
            "  Topic 5: 0.0242\n",
            "Document 84:\n",
            "  Topic 1: 0.0380\n",
            "  Topic 2: 0.0381\n",
            "  Topic 3: 0.8472\n",
            "  Topic 4: 0.0383\n",
            "  Topic 5: 0.0383\n",
            "Document 85:\n",
            "  Topic 1: 0.0424\n",
            "  Topic 2: 0.0427\n",
            "  Topic 3: 0.8300\n",
            "  Topic 4: 0.0425\n",
            "  Topic 5: 0.0424\n",
            "Document 86:\n",
            "  Topic 1: 0.0248\n",
            "  Topic 2: 0.0249\n",
            "  Topic 3: 0.0246\n",
            "  Topic 4: 0.9010\n",
            "  Topic 5: 0.0247\n",
            "Document 87:\n",
            "  Topic 1: 0.0359\n",
            "  Topic 2: 0.8570\n",
            "  Topic 3: 0.0356\n",
            "  Topic 4: 0.0358\n",
            "  Topic 5: 0.0357\n",
            "Document 88:\n",
            "  Topic 1: 0.8505\n",
            "  Topic 2: 0.0376\n",
            "  Topic 3: 0.0372\n",
            "  Topic 4: 0.0373\n",
            "  Topic 5: 0.0373\n",
            "Document 89:\n",
            "  Topic 1: 0.0295\n",
            "  Topic 2: 0.0295\n",
            "  Topic 3: 0.8824\n",
            "  Topic 4: 0.0295\n",
            "  Topic 5: 0.0291\n",
            "Document 90:\n",
            "  Topic 1: 0.0317\n",
            "  Topic 2: 0.8733\n",
            "  Topic 3: 0.0311\n",
            "  Topic 4: 0.0316\n",
            "  Topic 5: 0.0324\n",
            "Document 91:\n",
            "  Topic 1: 0.0331\n",
            "  Topic 2: 0.8692\n",
            "  Topic 3: 0.0324\n",
            "  Topic 4: 0.0326\n",
            "  Topic 5: 0.0328\n",
            "Document 92:\n",
            "  Topic 1: 0.0292\n",
            "  Topic 2: 0.0292\n",
            "  Topic 3: 0.0290\n",
            "  Topic 4: 0.0293\n",
            "  Topic 5: 0.8833\n",
            "Document 93:\n",
            "  Topic 1: 0.0376\n",
            "  Topic 2: 0.0374\n",
            "  Topic 3: 0.0372\n",
            "  Topic 4: 0.8504\n",
            "  Topic 5: 0.0374\n",
            "Document 94:\n",
            "  Topic 1: 0.0283\n",
            "  Topic 2: 0.0287\n",
            "  Topic 3: 0.0281\n",
            "  Topic 4: 0.8865\n",
            "  Topic 5: 0.0283\n",
            "Document 95:\n",
            "  Topic 1: 0.0262\n",
            "  Topic 2: 0.0266\n",
            "  Topic 3: 0.0262\n",
            "  Topic 4: 0.0262\n",
            "  Topic 5: 0.8949\n",
            "Document 96:\n",
            "  Topic 1: 0.0429\n",
            "  Topic 2: 0.0426\n",
            "  Topic 3: 0.8295\n",
            "  Topic 4: 0.0426\n",
            "  Topic 5: 0.0425\n",
            "Document 97:\n",
            "  Topic 1: 0.0449\n",
            "  Topic 2: 0.0453\n",
            "  Topic 3: 0.0441\n",
            "  Topic 4: 0.8204\n",
            "  Topic 5: 0.0453\n",
            "Document 98:\n",
            "  Topic 1: 0.0343\n",
            "  Topic 2: 0.0338\n",
            "  Topic 3: 0.8642\n",
            "  Topic 4: 0.0338\n",
            "  Topic 5: 0.0339\n",
            "Document 99:\n",
            "  Topic 1: 0.0411\n",
            "  Topic 2: 0.8354\n",
            "  Topic 3: 0.0408\n",
            "  Topic 4: 0.0410\n",
            "  Topic 5: 0.0416\n",
            "Document 100:\n",
            "  Topic 1: 0.0312\n",
            "  Topic 2: 0.0313\n",
            "  Topic 3: 0.8764\n",
            "  Topic 4: 0.0307\n",
            "  Topic 5: 0.0305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assign the dominant topic to each document\n",
        "dominant_topics = np.argmax(lda_topics, axis=1)\n",
        "\n",
        "# Print the dominant topic for each document\n",
        "for idx, topic in enumerate(dominant_topics):\n",
        "    print(f\"Document {idx + 1}: Dominant Topic {topic + 1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbiPweY8vvaR",
        "outputId": "391f070f-46c1-47c5-a610-62f1cd689a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: Dominant Topic 1\n",
            "Document 2: Dominant Topic 1\n",
            "Document 3: Dominant Topic 4\n",
            "Document 4: Dominant Topic 5\n",
            "Document 5: Dominant Topic 2\n",
            "Document 6: Dominant Topic 2\n",
            "Document 7: Dominant Topic 1\n",
            "Document 8: Dominant Topic 3\n",
            "Document 9: Dominant Topic 4\n",
            "Document 10: Dominant Topic 2\n",
            "Document 11: Dominant Topic 5\n",
            "Document 12: Dominant Topic 5\n",
            "Document 13: Dominant Topic 5\n",
            "Document 14: Dominant Topic 1\n",
            "Document 15: Dominant Topic 5\n",
            "Document 16: Dominant Topic 5\n",
            "Document 17: Dominant Topic 1\n",
            "Document 18: Dominant Topic 4\n",
            "Document 19: Dominant Topic 3\n",
            "Document 20: Dominant Topic 5\n",
            "Document 21: Dominant Topic 1\n",
            "Document 22: Dominant Topic 2\n",
            "Document 23: Dominant Topic 5\n",
            "Document 24: Dominant Topic 1\n",
            "Document 25: Dominant Topic 1\n",
            "Document 26: Dominant Topic 2\n",
            "Document 27: Dominant Topic 4\n",
            "Document 28: Dominant Topic 2\n",
            "Document 29: Dominant Topic 2\n",
            "Document 30: Dominant Topic 3\n",
            "Document 31: Dominant Topic 1\n",
            "Document 32: Dominant Topic 5\n",
            "Document 33: Dominant Topic 5\n",
            "Document 34: Dominant Topic 4\n",
            "Document 35: Dominant Topic 1\n",
            "Document 36: Dominant Topic 2\n",
            "Document 37: Dominant Topic 1\n",
            "Document 38: Dominant Topic 1\n",
            "Document 39: Dominant Topic 3\n",
            "Document 40: Dominant Topic 2\n",
            "Document 41: Dominant Topic 1\n",
            "Document 42: Dominant Topic 2\n",
            "Document 43: Dominant Topic 2\n",
            "Document 44: Dominant Topic 2\n",
            "Document 45: Dominant Topic 1\n",
            "Document 46: Dominant Topic 4\n",
            "Document 47: Dominant Topic 1\n",
            "Document 48: Dominant Topic 5\n",
            "Document 49: Dominant Topic 5\n",
            "Document 50: Dominant Topic 4\n",
            "Document 51: Dominant Topic 5\n",
            "Document 52: Dominant Topic 4\n",
            "Document 53: Dominant Topic 4\n",
            "Document 54: Dominant Topic 2\n",
            "Document 55: Dominant Topic 1\n",
            "Document 56: Dominant Topic 3\n",
            "Document 57: Dominant Topic 4\n",
            "Document 58: Dominant Topic 1\n",
            "Document 59: Dominant Topic 2\n",
            "Document 60: Dominant Topic 5\n",
            "Document 61: Dominant Topic 3\n",
            "Document 62: Dominant Topic 5\n",
            "Document 63: Dominant Topic 2\n",
            "Document 64: Dominant Topic 1\n",
            "Document 65: Dominant Topic 1\n",
            "Document 66: Dominant Topic 3\n",
            "Document 67: Dominant Topic 4\n",
            "Document 68: Dominant Topic 5\n",
            "Document 69: Dominant Topic 4\n",
            "Document 70: Dominant Topic 2\n",
            "Document 71: Dominant Topic 2\n",
            "Document 72: Dominant Topic 2\n",
            "Document 73: Dominant Topic 1\n",
            "Document 74: Dominant Topic 5\n",
            "Document 75: Dominant Topic 4\n",
            "Document 76: Dominant Topic 4\n",
            "Document 77: Dominant Topic 4\n",
            "Document 78: Dominant Topic 1\n",
            "Document 79: Dominant Topic 1\n",
            "Document 80: Dominant Topic 1\n",
            "Document 81: Dominant Topic 5\n",
            "Document 82: Dominant Topic 2\n",
            "Document 83: Dominant Topic 2\n",
            "Document 84: Dominant Topic 3\n",
            "Document 85: Dominant Topic 3\n",
            "Document 86: Dominant Topic 4\n",
            "Document 87: Dominant Topic 2\n",
            "Document 88: Dominant Topic 1\n",
            "Document 89: Dominant Topic 3\n",
            "Document 90: Dominant Topic 2\n",
            "Document 91: Dominant Topic 2\n",
            "Document 92: Dominant Topic 5\n",
            "Document 93: Dominant Topic 4\n",
            "Document 94: Dominant Topic 4\n",
            "Document 95: Dominant Topic 5\n",
            "Document 96: Dominant Topic 3\n",
            "Document 97: Dominant Topic 4\n",
            "Document 98: Dominant Topic 3\n",
            "Document 99: Dominant Topic 2\n",
            "Document 100: Dominant Topic 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyLDAvis==3.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQABxcxcv2_W",
        "outputId": "f787d65f-4a05-48cc-bdc8-ba856f2366b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis==3.3.1\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.3.1) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.3.1) (1.11.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.3.1) (2.0.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.3.1) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.3.1) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.3.1) (2.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.3.1) (0.18.3)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis==3.3.1) (2.0)\n",
            "Collecting sklearn (from pyLDAvis==3.3.1)\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create a Gensim dictionary and corpus from your data\n",
        "texts = [doc.split() for doc in combined_texts]\n",
        "id2word = corpora.Dictionary(texts)\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# Train the LDA model using Gensim\n",
        "lda_model_gensim = gensim.models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=n_topics,\n",
        "    random_state=42,\n",
        "    update_every=1,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "# Prepare the pyLDAvis visualization\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = gensimvis.prepare(lda_model_gensim, corpus, id2word)\n",
        "pyLDAvis.display(panel)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "cUPvrHj8wMmH",
        "outputId": "630e5683-d619-4ea5-ba93-a49f27a55efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el14781375744792522721984962071\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el14781375744792522721984962071_data = {\"mdsDat\": {\"x\": [-0.011256059424756673, -0.029496865563706728, -0.07599216204632227, 0.14212522997991783, -0.025380142945132137], \"y\": [-0.12105703748063931, 0.11101795204689631, 0.003941253405028642, 0.014126857984678165, -0.008029025955963724], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [33.48681919582, 22.922087610910722, 15.893565251726876, 15.44448779172399, 12.253040149818416]}, \"tinfo\": {\"Term\": [\"r\", \"CI\", \"group\", \"cancer\", \"MitoEsc\", \"P\", \"Group\", \"survival\", \"HFpEF\", \"metastasis\", \"calcification\", \"COVID\", \"thyroid\", \"mortality\", \"level\", \"serum\", \"asprosin\", \"MICT\", \"PBMAs\", \"insulin\", \"foot\", \"distant\", \"adult\", \"glucose\", \"cell\", \"woman\", \"day\", \"OVX\", \"CGM\", \"pneumonia\", \"ulcer\", \"selfcare\", \"brain\", \"NTX\", \"DReSS\", \"observation\", \"ETS\", \"vein\", \"AD\", \"nursing\", \"corneal\", \"decreased\", \"venous\", \"islet\", \"GHNF\", \"ADSCs\", \"AG\", \"varicose\", \"visual\", \"tear\", \"lacrimal\", \"isolates\", \"DNA\", \"RAIA\", \"transplant\", \"Silicone\", \"C\", \"conventional\", \"gland\", \"Pg\", \"foot\", \"group\", \"control\", \"treatment\", \"diabetes\", \"care\", \"number\", \"effect\", \"patient\", \"day\", \"transplantation\", \"study\", \"approach\", \"P\", \"efficacy\", \"management\", \"also\", \"diabetic\", \"disease\", \"type\", \"level\", \"review\", \"wound\", \"activity\", \"cell\", \"higher\", \"rate\", \"kidney\", \"CGM\", \"valve\", \"VICs\", \"bread\", \"incidence\", \"pearl\", \"millet\", \"antiTB\", \"periodontitis\", \"sensor\", \"PCSK\", \"nomogram\", \"starch\", \"duration\", \"vasopressor\", \"accuracy\", \"MARD\", \"coagulation\", \"crosssectional\", \"ratio\", \"PCa\", \"prostate\", \"PMSGH\", \"germ\", \"TB\", \"KP\", \"glioblastoma\", \"receiving\", \"stem\", \"CI\", \"adverse\", \"induced\", \"calcification\", \"adult\", \"infusion\", \"risk\", \"drug\", \"cell\", \"glucose\", \"therapy\", \"patient\", \"study\", \"type\", \"analysis\", \"diabetes\", \"identify\", \"factor\", \"disease\", \"treatment\", \"P\", \"level\", \"cancer\", \"including\", \"may\", \"outcome\", \"effect\", \"HFpEF\", \"r\", \"semaglutide\", \"OPRM\", \"HTRB\", \"COMT\", \"lncRNAs\", \"ankle\", \"PJI\", \"PCOS\", \"FMS\", \"vaccination\", \"polymorphism\", \"neutrophil\", \"HS\", \"pain\", \"apheresis\", \"preserved\", \"ejection\", \"x\", \"psychophysical\", \"sensoryrelated\", \"Heart\", \"fraction\", \"sleep\", \"CCN\", \"Intraretinal\", \"HRF\", \"DME\", \"MASLD\", \"surgical\", \"mitochondrial\", \"cardiac\", \"dyslipidemia\", \"retinal\", \"gene\", \"three\", \"COVID\", \"woman\", \"cardiovascular\", \"hospitalized\", \"event\", \"inflammation\", \"disease\", \"dysfunction\", \"clinical\", \"diabetes\", \"patient\", \"outcome\", \"may\", \"type\", \"diabetic\", \"treatment\", \"study\", \"also\", \"pathway\", \"significant\", \"therapeutic\", \"MitoEsc\", \"Group\", \"serum\", \"asprosin\", \"MICT\", \"PBMAs\", \"OVX\", \"GDM\", \"pneumonia\", \"HIIT\", \"potassium\", \"magnesium\", \"electrolyte\", \"N\", \"PE\", \"CCI\", \"PESI\", \"atherosclerosis\", \"senescence\", \"ES\", \"conductive\", \"MetS\", \"meat\", \"W\", \"uric\", \"stone\", \"readmission\", \"HGinduced\", \"MTMAA\", \"detraining\", \"mean\", \"healthcare\", \"mortality\", \"level\", \"P\", \"insulin\", \"system\", \"COVID\", \"group\", \"study\", \"wound\", \"patient\", \"acid\", \"score\", \"cell\", \"glucose\", \"using\", \"day\", \"metastasis\", \"distant\", \"HSK\", \"GCDCO\", \"photodynamic\", \"Limb\", \"gangrene\", \"Fournier\", \"stay\", \"SGLT\", \"understudied\", \"QT\", \"lung\", \"dos\", \"corrected\", \"nanoagents\", \"bioreactor\", \"antibacterial\", \"survival\", \"differentiated\", \"tophus\", \"gouty\", \"senescent\", \"Benefits\", \"Burdens\", \"measure\", \"AIDBBS\", \"AID\", \"Delivery\", \"Automated\", \"thyroid\", \"cancer\", \"vascular\", \"tumor\", \"practice\", \"cascade\", \"affect\", \"property\", \"patient\", \"concentration\", \"study\", \"diabetic\", \"effect\", \"wound\", \"treatment\", \"calcification\", \"outcome\", \"inhibitor\", \"therapy\", \"significant\", \"disease\", \"clinical\", \"role\", \"diabetes\", \"adult\", \"glucose\"], \"Freq\": [13.0, 17.0, 41.0, 13.0, 8.0, 30.0, 7.0, 7.0, 7.0, 6.0, 12.0, 15.0, 8.0, 16.0, 28.0, 6.0, 6.0, 6.0, 6.0, 15.0, 19.0, 5.0, 15.0, 26.0, 26.0, 8.0, 12.0, 5.0, 7.0, 5.0, 9.90556959807312, 9.88329144757161, 9.110189129967564, 7.501682836118849, 8.22424492888869, 6.686877992970111, 6.685780813279337, 6.682501049494098, 6.677615690681575, 5.869483279326547, 5.8680002858177405, 5.866299103165883, 5.864157386244244, 5.061095274975371, 5.06079327349483, 5.057851529715334, 5.0574109954455535, 5.055945318535402, 5.052651355139266, 5.051661884692125, 5.049253838483174, 5.038829592424894, 4.252264658362576, 4.251657192081763, 4.248266255733158, 4.239534880359382, 4.239136598590319, 4.23890420983637, 4.237778977255775, 4.236660324982668, 15.538199211653652, 29.550152207044906, 13.283083504727353, 28.7785079470476, 31.970009721606623, 8.307708163749549, 6.6778027099470485, 18.175651563601342, 25.375882201687972, 8.342314347165459, 6.687254109492896, 18.12516882987467, 6.6820231113712225, 12.406121077038804, 7.556639486274004, 8.302034553366255, 9.176979943643481, 10.07625099904717, 10.785419049089413, 9.95455964099574, 9.20935921247237, 7.514440320679891, 7.492370662026187, 6.796027977611713, 6.725922766953246, 6.715110282753123, 6.705920017514992, 6.696400736902417, 6.884297020852382, 5.40385361746921, 5.402558752285363, 5.395866797981146, 4.654155598312608, 4.648633811446595, 4.6485773892727105, 4.648101830949972, 4.647553255696325, 4.64144448856755, 3.914148443122557, 3.898945275008293, 3.895668758767731, 3.893930813571323, 3.8929289643829414, 3.8931527565348185, 3.890045032679994, 3.16138063098433, 3.1575218758401564, 3.1558998568749637, 3.152202545009845, 3.152217717359125, 3.1491612001831197, 3.1476707539343383, 3.1476932753903, 3.145515332064778, 3.1407993391862963, 5.3938545651579135, 6.137280240295068, 14.398876189311125, 4.6548089576034695, 8.387354336701165, 8.396538349374588, 9.911390270244745, 4.6448302931365175, 12.336595475436043, 10.665454132906607, 12.904327716592814, 12.91463163829745, 12.151407469757917, 24.499319621213022, 19.10317497958569, 10.713943064661153, 7.6916893205531265, 11.56681685200764, 4.718275368776742, 6.950743376251388, 7.726766369680254, 8.46255893072193, 6.938554290148738, 6.1978364052221115, 5.438892735723138, 5.411414188769695, 5.415918005826217, 5.412942328823719, 5.442643624946651, 6.941625547875401, 12.381890372160369, 4.223448559883383, 4.218946284540307, 4.217581958678768, 4.217343283841304, 4.18570703276138, 3.54127017771295, 3.539839443701043, 3.537391547227546, 3.5349982237885103, 3.5317968249355802, 2.855540603263883, 3.436029039532869, 2.821471907111785, 4.898031098913854, 2.1809725293750764, 2.179167016692771, 2.1789931062395773, 2.177912330029985, 2.1772817813547007, 2.1767643237869754, 2.177498265349839, 2.1763721211959357, 2.1730944515960475, 2.126657730567581, 1.5031599899615664, 1.5031580174422485, 1.50315653805276, 1.502249343543142, 2.858057537913506, 2.859850557973465, 5.577507458372001, 2.8586512662281898, 4.149530370964901, 4.220257352380262, 2.851109009863031, 6.2436153969063914, 4.219991719778786, 4.218530740470672, 3.535901308882883, 4.2211591224617555, 3.523866311018017, 8.972064607999524, 3.5327225940021085, 5.589557578884969, 8.992012038354872, 9.693614804508192, 5.59814329796929, 4.901461967514098, 4.914018039232104, 4.818546460480296, 4.910816311625955, 4.861594064567413, 4.185658377284872, 3.550685012417223, 3.5411856881355006, 3.5400396544118085, 7.538987066133611, 6.853716553180831, 5.515804961600232, 5.514640834060235, 5.512605208186235, 5.509944710339613, 4.8448259990396805, 4.830264181935367, 4.829583403727976, 4.169139731511553, 4.164032137903028, 4.158802508970607, 4.155185535313132, 4.149042238772481, 3.5066997275175673, 3.505881579704743, 3.505585436392565, 3.50244095028494, 3.501885721507631, 3.497981230071342, 3.497633972487375, 3.4971122874509275, 3.49368731177686, 3.488087423838167, 2.8356217310554244, 2.834374223359572, 2.8295356186063376, 2.8282309268189207, 2.826374520360381, 2.8251158314175746, 5.503870418065877, 3.4884049712516565, 7.5540395802471005, 10.20054297818168, 10.241592849855497, 6.863173971318607, 6.1931355113589746, 5.490744273070439, 7.549237394412516, 7.550311433048484, 5.5224709015699265, 7.556968428020616, 4.185646446657441, 4.185217406238611, 4.865770391369458, 4.854214093446343, 4.177226328772785, 4.158516588512787, 5.694629350631593, 4.45136363217538, 3.215196038448513, 3.1747785974550182, 2.5738145974598634, 1.9897849955527434, 1.9865518587924105, 1.9863086733793631, 1.9856645298387066, 1.9842672591955273, 1.983590293861239, 1.9793424694499053, 1.979143764537108, 1.9790764735239093, 1.9767970696549755, 1.967388366689001, 1.9603148266236126, 3.814108866429293, 5.688798983521908, 3.212586008866895, 1.3700210776855666, 1.36998990331787, 1.3695253292041483, 1.369469443447424, 1.369427117273397, 1.3693837772987945, 1.3693377760976813, 1.36929190162164, 1.3692394374418582, 1.369011839212659, 5.083575753858945, 6.931346931170667, 2.601631257650358, 2.595377882250547, 3.8539578187500774, 1.9667197652094606, 2.6106365947123567, 2.5986552460609857, 11.898352853457208, 2.5955748130123375, 7.603846980824207, 5.111869914120499, 5.711656638197358, 4.501434735698904, 5.108996803289539, 3.2209866138856094, 3.860369853939954, 3.22555936138139, 3.8604988600631858, 3.223592841715069, 3.866162456978201, 3.2437710214694797, 2.6025482402708953, 3.2584632728597303, 2.6139770676086203, 2.603034864547134], \"Total\": [13.0, 17.0, 41.0, 13.0, 8.0, 30.0, 7.0, 7.0, 7.0, 6.0, 12.0, 15.0, 8.0, 16.0, 28.0, 6.0, 6.0, 6.0, 6.0, 15.0, 19.0, 5.0, 15.0, 26.0, 26.0, 8.0, 12.0, 5.0, 7.0, 5.0, 10.525833034939001, 10.526376676403045, 9.705707260128682, 8.074198377633152, 8.884993793784538, 7.256013196681103, 7.256344692470065, 7.255865564015352, 7.255847303793681, 6.437210539405231, 6.4373186655056145, 6.437785967620723, 6.437192785340659, 5.619621684605569, 5.619535122355879, 5.618805147827372, 5.618714237347879, 5.619821751337944, 5.619213615352019, 5.618661830118227, 5.618404115633516, 5.617692690585127, 4.803232335842963, 4.803066749770618, 4.802517841586623, 4.800133886042049, 4.801182480810911, 4.801502901081058, 4.801041521570852, 4.801685294537364, 19.25996250342923, 41.1137711777157, 18.755539051701867, 48.761791482894836, 57.94918208666564, 11.15009275147062, 8.759795023504287, 34.36408994864094, 79.02413790888701, 12.955123202798678, 9.122819858286459, 57.24409628790047, 9.374639051043648, 30.52872169648517, 12.181021087377447, 15.101326901881157, 20.16139555921019, 26.110511269603144, 32.85508726221768, 28.398184211975575, 28.6261093866808, 15.926966702335674, 19.96419799940799, 11.86860582889773, 26.034949975560387, 13.599165440481313, 13.556720512510262, 10.555492829054877, 7.48297423709779, 5.9794397606183525, 5.9795349089143786, 5.977335223321734, 5.225977106850169, 5.22506976102718, 5.225115222232119, 5.225315015560477, 5.22660637325648, 5.225442639486855, 4.474459139005988, 4.473728620983257, 4.472098951786927, 4.473050000004786, 4.472734973125949, 4.47302478769544, 4.4725903148730675, 3.7216818878515783, 3.7211753514825823, 3.721185198338851, 3.720680125817145, 3.720747034705756, 3.7202985586564354, 3.719974353941231, 3.7203004868324387, 3.72096184381826, 3.7198612996139144, 6.601160309755843, 7.549650953250426, 17.743965114216707, 5.849271297619119, 11.791687069862423, 12.108832912185523, 15.302146298024205, 6.042292465609103, 21.3433128093665, 18.137312990327104, 26.034949975560387, 26.47035171623105, 26.16517834059039, 79.02413790888701, 57.24409628790047, 28.398184211975575, 16.70609962085893, 57.94918208666564, 7.285645286742975, 20.540506760349835, 32.85508726221768, 48.761791482894836, 30.52872169648517, 28.6261093866808, 13.523500130537482, 14.242768236058149, 18.67052633865617, 21.844569167392972, 34.36408994864094, 7.548281777970507, 13.763913804720234, 4.811994623047584, 4.810834738833606, 4.810935559663684, 4.810888042335813, 4.818672282775813, 4.129045503754495, 4.129249779292197, 4.126778251409476, 4.127492163553253, 4.12975794694776, 3.4441320245581433, 4.149079997576389, 3.452129602539527, 6.117697584152156, 2.7607123848281, 2.761818996734845, 2.7617948430766344, 2.760746943272174, 2.760776979226252, 2.760766734126216, 2.7619025642949344, 2.7623659186089733, 2.7612858452735454, 2.7714557053819, 2.0773381852133146, 2.0773364529106626, 2.077336389744969, 2.0774286585032384, 4.068016217842402, 4.122669235842364, 8.567380148085734, 4.1960786144870745, 6.3312003893852555, 7.267376206622357, 4.262629495103166, 15.297087945186894, 8.193665592305166, 8.372812720365815, 6.236137312227451, 8.511737725291011, 6.303082786225936, 32.85508726221768, 6.444044327141683, 18.563647543025677, 57.94918208666564, 79.02413790888701, 21.844569167392972, 18.67052633865617, 28.398184211975575, 26.110511269603144, 48.761791482894836, 57.24409628790047, 20.16139555921019, 11.622111768595497, 15.638473153446279, 10.885329860574215, 8.155232970930323, 7.478769776190293, 6.124306061422082, 6.124268369036651, 6.12382609677458, 6.1252400902236985, 5.447618716446935, 5.449288659526543, 5.451302740222006, 4.770731944704654, 4.773628098626051, 4.773941910317075, 4.775189360495155, 4.776040796367432, 4.0933226468993515, 4.093467285896663, 4.093387090509038, 4.094480922690854, 4.094682872539966, 4.09378838523094, 4.094047638639121, 4.094263691984089, 4.0952632640696125, 4.094993067269061, 3.416794841031492, 3.4167515531463897, 3.4168050895730926, 3.417465196336088, 3.4173903029173194, 3.417136157362683, 8.187704507714844, 4.717734489460112, 16.002075553477493, 28.6261093866808, 30.52872169648517, 15.397898250302468, 13.642697090845013, 15.297087945186894, 41.1137711777157, 57.24409628790047, 19.96419799940799, 79.02413790888701, 10.033774343643634, 10.2272419577988, 26.034949975560387, 26.47035171623105, 11.741698316361623, 12.955123202798678, 6.322739264021861, 5.07674290267067, 3.829293313566747, 3.842561169015603, 3.2141085780060776, 2.58018063902525, 2.580491894127261, 2.5806030769151214, 2.5806792837521666, 2.5808237033953563, 2.5811895050459044, 2.581745844318541, 2.581652221836844, 2.5819261986302475, 2.5822390175251426, 2.5859876929330836, 2.5880831480667306, 5.13662148852984, 7.9613041231430195, 4.582157957868423, 1.9563739101088449, 1.956384621990761, 1.9564042789706657, 1.9565472598357392, 1.9565608137605404, 1.9565736984280844, 1.9565884934940159, 1.9566022172639643, 1.956617747308425, 1.9566882632876978, 8.150927722811666, 13.523500130537482, 4.6994711670389915, 4.7757487906574205, 8.398851368998713, 3.3386627759762115, 5.461844426366129, 5.519143493859154, 79.02413790888701, 6.140456742382904, 57.24409628790047, 26.110511269603144, 34.36408994864094, 19.96419799940799, 48.761791482894836, 12.108832912185523, 21.844569167392972, 13.109754952849272, 26.16517834059039, 15.638473153446279, 32.85508726221768, 18.563647543025677, 11.125888697843658, 57.94918208666564, 15.302146298024205, 26.47035171623105], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.7049, -5.7071, -5.7886, -5.9829, -5.8909, -6.0978, -6.098, -6.0985, -6.0992, -6.2282, -6.2285, -6.2288, -6.2291, -6.3764, -6.3765, -6.377, -6.3771, -6.3774, -6.3781, -6.3783, -6.3787, -6.3808, -6.5505, -6.5507, -6.5515, -6.5535, -6.5536, -6.5537, -6.5539, -6.5542, -5.2547, -4.6119, -5.4115, -4.6384, -4.5332, -5.8808, -6.0992, -5.0979, -4.7642, -5.8766, -6.0978, -5.1007, -6.0986, -5.4798, -5.9756, -5.8815, -5.7813, -5.6878, -5.6198, -5.7, -5.7778, -5.9812, -5.9841, -6.0817, -6.092, -6.0936, -6.095, -6.0964, -5.6897, -5.9318, -5.9321, -5.9333, -6.0812, -6.0824, -6.0824, -6.0825, -6.0826, -6.0839, -6.2543, -6.2582, -6.2591, -6.2595, -6.2598, -6.2597, -6.2605, -6.4679, -6.4692, -6.4697, -6.4708, -6.4708, -6.4718, -6.4723, -6.4723, -6.473, -6.4745, -5.9337, -5.8046, -4.9518, -6.081, -5.4922, -5.4911, -5.3253, -6.0832, -5.1064, -5.2519, -5.0614, -5.0606, -5.1215, -4.4203, -4.6691, -5.2474, -5.5788, -5.1708, -6.0675, -5.6801, -5.5742, -5.4833, -5.6818, -5.7947, -5.9254, -5.9304, -5.9296, -5.9301, -5.9247, -5.3152, -4.7365, -5.8121, -5.8132, -5.8135, -5.8135, -5.8211, -5.9883, -5.9887, -5.9894, -5.99, -5.9909, -6.2035, -6.0184, -6.2155, -5.6639, -6.473, -6.4738, -6.4739, -6.4744, -6.4747, -6.4749, -6.4746, -6.4751, -6.4766, -6.4982, -6.8452, -6.8452, -6.8452, -6.8458, -6.2026, -6.202, -5.534, -6.2024, -5.8298, -5.8129, -6.205, -5.4212, -5.8129, -5.8133, -5.9898, -5.8126, -5.9932, -5.0586, -5.9907, -5.5319, -5.0564, -4.9813, -5.5303, -5.6632, -5.6607, -5.6803, -5.6613, -5.6714, -5.8211, -5.9856, -5.9883, -5.9886, -5.204, -5.2993, -5.5165, -5.5167, -5.5171, -5.5175, -5.6462, -5.6492, -5.6493, -5.7964, -5.7976, -5.7989, -5.7997, -5.8012, -5.9694, -5.9696, -5.9697, -5.9706, -5.9708, -5.9719, -5.972, -5.9722, -5.9731, -5.9747, -6.1818, -6.1823, -6.184, -6.1844, -6.1851, -6.1855, -5.5186, -5.9746, -5.202, -4.9016, -4.8976, -5.2979, -5.4006, -5.521, -5.2026, -5.2025, -5.5153, -5.2016, -5.7924, -5.7925, -5.6419, -5.6442, -5.7944, -5.7989, -5.2531, -5.4994, -5.8247, -5.8374, -6.0472, -6.3046, -6.3062, -6.3063, -6.3067, -6.3074, -6.3077, -6.3098, -6.3099, -6.31, -6.3111, -6.3159, -6.3195, -5.6539, -5.2541, -5.8255, -6.6778, -6.6778, -6.6781, -6.6782, -6.6782, -6.6783, -6.6783, -6.6783, -6.6784, -6.6785, -5.3666, -5.0566, -6.0365, -6.0389, -5.6435, -6.3162, -6.033, -6.0376, -4.5162, -6.0388, -4.964, -5.361, -5.2501, -5.4882, -5.3616, -5.8229, -5.6418, -5.8215, -5.6418, -5.8221, -5.6403, -5.8159, -6.0361, -5.8114, -6.0317, -6.0359], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0333, 1.031, 1.0307, 1.0205, 1.0167, 1.0123, 1.0121, 1.0117, 1.011, 1.0017, 1.0014, 1.0011, 1.0008, 0.9893, 0.9893, 0.9888, 0.9888, 0.9883, 0.9877, 0.9876, 0.9872, 0.9853, 0.9722, 0.9721, 0.9714, 0.9698, 0.9695, 0.9694, 0.9692, 0.9688, 0.8793, 0.7638, 0.749, 0.5667, 0.4993, 0.7998, 0.8226, 0.4571, -0.0419, 0.6539, 0.7834, -0.056, 0.7554, 0.1935, 0.6166, 0.4957, 0.3069, 0.1419, -0.0199, 0.0457, -0.0401, 0.3428, 0.114, 0.5365, -0.2595, 0.3884, 0.3901, 0.6389, 1.3897, 1.3719, 1.3716, 1.3707, 1.3572, 1.3562, 1.3562, 1.356, 1.3556, 1.3546, 1.3393, 1.3356, 1.3351, 1.3344, 1.3342, 1.3342, 1.3335, 1.3099, 1.3088, 1.3083, 1.3073, 1.3073, 1.3064, 1.306, 1.3059, 1.3051, 1.3039, 1.2711, 1.2659, 1.2642, 1.2447, 1.1324, 1.107, 1.0388, 1.21, 0.9249, 0.9421, 0.7712, 0.7554, 0.7061, 0.302, 0.3756, 0.4983, 0.6974, -0.1384, 1.0386, 0.3895, 0.0257, -0.2782, -0.0085, -0.057, 0.5622, 0.5053, 0.2355, 0.0779, -0.3697, 1.7555, 1.7334, 1.7088, 1.708, 1.7076, 1.7076, 1.6984, 1.6857, 1.6852, 1.6851, 1.6843, 1.6828, 1.6518, 1.6507, 1.6375, 1.6169, 1.6035, 1.6023, 1.6022, 1.6021, 1.6018, 1.6016, 1.6015, 1.6008, 1.5997, 1.5744, 1.5157, 1.5157, 1.5157, 1.5151, 1.4862, 1.4735, 1.41, 1.4555, 1.4168, 1.2958, 1.4371, 0.9432, 1.1757, 1.1538, 1.2719, 1.1379, 1.2578, 0.5413, 1.2382, 0.639, -0.024, -0.259, 0.4777, 0.5018, 0.085, 0.1494, -0.4563, -0.6267, 0.2672, 0.6535, 0.354, 0.716, 1.7893, 1.7806, 1.7633, 1.7631, 1.7628, 1.7621, 1.7507, 1.7473, 1.7468, 1.7331, 1.7313, 1.73, 1.7288, 1.7272, 1.7132, 1.713, 1.7129, 1.7117, 1.7115, 1.7106, 1.7105, 1.7103, 1.709, 1.7075, 1.6815, 1.681, 1.6793, 1.6787, 1.678, 1.6777, 1.4707, 1.566, 1.1173, 0.836, 0.7757, 1.0599, 1.0782, 0.8433, 0.173, -0.1578, 0.5828, -0.4794, 0.9936, 0.9744, 0.1907, 0.1717, 0.8344, 0.7316, 1.9948, 1.9679, 1.9246, 1.9085, 1.8772, 1.8396, 1.8378, 1.8377, 1.8373, 1.8365, 1.8361, 1.8337, 1.8336, 1.8335, 1.8322, 1.826, 1.8216, 1.8017, 1.7633, 1.7443, 1.7431, 1.7431, 1.7428, 1.7426, 1.7426, 1.7426, 1.7425, 1.7425, 1.7424, 1.7422, 1.6273, 1.431, 1.5081, 1.4896, 1.3204, 1.5702, 1.3612, 1.3462, 0.206, 1.2383, 0.0807, 0.4686, 0.3049, 0.6099, -0.1565, 0.7751, 0.3662, 0.6971, 0.1858, 0.5202, -0.0404, 0.3549, 0.6466, -0.7789, 0.3323, -0.22]}, \"token.table\": {\"Topic\": [1, 1, 1, 5, 5, 5, 5, 5, 1, 4, 3, 2, 2, 3, 3, 2, 3, 4, 3, 1, 1, 5, 4, 1, 3, 5, 5, 4, 1, 4, 3, 4, 4, 3, 3, 5, 3, 3, 3, 2, 5, 2, 3, 4, 4, 4, 4, 4, 1, 3, 4, 1, 2, 4, 5, 4, 3, 2, 2, 4, 4, 3, 2, 1, 5, 1, 5, 1, 2, 2, 4, 2, 1, 2, 3, 4, 1, 3, 4, 5, 1, 2, 4, 5, 2, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 2, 4, 5, 3, 1, 2, 3, 4, 4, 5, 1, 2, 2, 5, 2, 3, 5, 1, 2, 3, 1, 3, 4, 5, 1, 2, 2, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 2, 2, 4, 5, 4, 1, 2, 3, 5, 1, 1, 5, 2, 1, 4, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 5, 5, 1, 2, 3, 4, 5, 2, 1, 3, 4, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 3, 2, 1, 2, 1, 2, 4, 5, 5, 1, 2, 4, 5, 4, 5, 1, 2, 4, 5, 2, 3, 4, 1, 2, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 1, 2, 4, 5, 1, 2, 4, 1, 1, 1, 2, 4, 5, 1, 1, 2, 3, 4, 3, 5, 4, 1, 2, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 5, 4, 5, 2, 3, 4, 1, 2, 4, 5, 3, 2, 1, 2, 1, 1, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 2, 2, 5, 4, 3, 4, 1, 4, 5, 3, 1, 4, 5, 2, 3, 2, 3, 1, 2, 3, 4, 5, 2, 4, 2, 5, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 3, 4, 5, 2, 3, 4, 1, 2, 3, 4, 5, 3, 2, 5, 1, 2, 4, 1, 2, 3, 4, 5, 3, 5, 1, 5, 1, 2, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 5, 5, 1, 1, 5, 1, 2, 3, 4, 5, 1, 2, 5, 1, 2, 3, 4, 5, 1, 5, 4, 1, 2, 4, 5, 3, 2, 1, 1, 4, 5, 2, 1, 1, 1, 3, 4, 1, 2, 3, 4, 5, 3], \"Freq\": [0.9647391554588101, 0.8898689077932084, 0.8898833058219523, 0.5110900883054097, 0.5110936731587492, 0.5110676129470743, 0.5111044443076496, 0.5111009036708573, 0.8331280920870991, 0.9771667197100393, 0.7216424192225741, 0.9354569156868957, 0.7890006495100134, 0.1690715677521457, 0.8314473263148094, 0.196115758159312, 0.392231516318624, 0.3268595969321867, 0.9627713690826629, 0.8327725415551867, 0.9003945512709719, 0.5110860316868875, 0.7328175561841512, 0.9646730270771074, 0.9691114704761794, 0.7750126386700352, 0.7807292761375995, 0.9175509525007289, 0.889753314310428, 0.935982816623862, 0.9273633663795309, 0.8778436143889167, 0.8384457660506079, 0.9627713398076164, 0.8690287866924458, 0.7834343713946759, 0.8314391141584166, 0.7241385072215845, 0.9627705369478042, 0.8062431505402253, 0.7751395269579138, 0.8943363282566873, 0.9627286077014915, 0.9797796190130548, 0.8778628526683048, 0.7327324827351785, 0.9809652315901143, 0.8375137840200876, 0.9908104341554582, 0.8314565386567002, 0.9178322236291011, 0.3930724685855937, 0.22929227334159633, 0.32756039048799473, 0.03275603904879947, 0.9795534398033491, 0.9692791219479322, 0.8939627954427158, 0.8063041966933754, 0.9772012482402181, 0.9771858638227578, 0.9686989680449043, 0.8063868941430961, 0.8330408501678772, 0.7746695920519262, 0.8328012514485728, 0.7749463852834197, 0.8333100898771388, 0.8063864762048505, 0.8361854351825803, 0.732601972877256, 0.8942494597846509, 0.09966339343015991, 0.39865357372063964, 0.09966339343015991, 0.39865357372063964, 0.589791261156923, 0.08425589445098901, 0.08425589445098901, 0.16851178890197802, 0.13070061944566808, 0.6535030972283403, 0.06535030972283404, 0.19605092916850209, 0.8548073333570961, 0.17096146667141923, 0.18308833462422863, 0.18308833462422863, 0.18308833462422863, 0.549265003872686, 0.4463976699216435, 0.14879922330721448, 0.19839896440961932, 0.14879922330721448, 0.04959974110240483, 0.17957512932906586, 0.4788670115441756, 0.17957512932906586, 0.1197167528860439, 0.05985837644302195, 0.9687468923175695, 0.9568801086844505, 0.1946804922716257, 0.7787219690865028, 0.7244506928687297, 0.7466954153526277, 0.10667077362180395, 0.2133415472436079, 0.9797088629125182, 0.9769248106231834, 0.7727726991669406, 0.9272894554498108, 0.8364931550921771, 0.6606747370301338, 0.24775302638630017, 0.36972676834671486, 0.07394535366934298, 0.5176174756854008, 0.23344359249039195, 0.11672179624519598, 0.7003307774711759, 0.23886835485227698, 0.47773670970455395, 0.11943417742613849, 0.11943417742613849, 0.7174828208442352, 0.1793707052110588, 0.2995211158178753, 0.5990422316357507, 0.2688693470343159, 0.4993287873494438, 0.19204953359593993, 0.03840990671918799, 0.2693436184031887, 0.16160617104191324, 0.3232123420838265, 0.05386872368063775, 0.16160617104191324, 0.8060871644598876, 0.3257086702029054, 0.1628543351014527, 0.4885630053043581, 0.7327711508987749, 0.6931285720001946, 0.1599527473846603, 0.05331758246155344, 0.05331758246155344, 0.8330724946764898, 0.9320650897944531, 0.7745216404935398, 0.806196891206631, 0.6175163195878963, 0.30875815979394816, 0.931997433617303, 0.8779281427039696, 0.5522079664928238, 0.2070779874348089, 0.15530849057610668, 0.034512997905801486, 0.051769496858702226, 0.38298752164388356, 0.15319500865755342, 0.19149376082194178, 0.07659750432877671, 0.19149376082194178, 0.2182377842917468, 0.6547133528752405, 0.33480355453658023, 0.243493494208422, 0.27393018098447475, 0.0608733735521055, 0.121746747104211, 0.7879067497973476, 0.7746154793506613, 0.2756747927692803, 0.6064845440924167, 0.05513495855385606, 0.05513495855385606, 0.05513495855385606, 0.8942444193549636, 0.3103640972139493, 0.6207281944278986, 0.15518204860697465, 0.23831774660929209, 0.7149532398278762, 0.5238026098436481, 0.14550072495656893, 0.08730043497394135, 0.05820028998262757, 0.1746008699478827, 0.6567593917303025, 0.08209492396628781, 0.16418984793257563, 0.08209492396628781, 0.08209492396628781, 0.7241667515650814, 0.8376631161670262, 0.11748482299080833, 0.23496964598161665, 0.4699392919632333, 0.11748482299080833, 0.292105743543876, 0.34079003413452197, 0.146052871771938, 0.09736858118129199, 0.09736858118129199, 0.8307388966697732, 0.15576354312558247, 0.7240170415247257, 0.7750460307787221, 0.4128037292559956, 0.5504049723413275, 0.8064571727010875, 0.8331525528425842, 0.8064816826131047, 0.22666869198874035, 0.4911154993089374, 0.18889057665728362, 0.11333434599437017, 0.5111469333583437, 0.729682516116655, 0.04864550107444367, 0.19458200429777467, 0.024322750537221834, 0.635898439537515, 0.211966146512505, 0.514737469048119, 0.22060177244919388, 0.22060177244919388, 0.0735339241497313, 0.16035567370193385, 0.6414226948077354, 0.16035567370193385, 0.1372562018383751, 0.6862810091918755, 0.1372562018383751, 0.9567588793004929, 0.351055350837037, 0.351055350837037, 0.1404221403348148, 0.0702110701674074, 0.0702110701674074, 0.0848055069707398, 0.6784440557659184, 0.0848055069707398, 0.0848055069707398, 0.0848055069707398, 0.15865252510807729, 0.6346101004323091, 0.15865252510807729, 0.16550009879390923, 0.8275004939695462, 0.30511630571177384, 0.3813953821397173, 0.07627907642794346, 0.2288372292838304, 0.32471964152002253, 0.19483178491201353, 0.4546074981280316, 0.8897396089307995, 0.8900451262454534, 0.6631618355830733, 0.09473740508329619, 0.09473740508329619, 0.18947481016659237, 0.8899324251324726, 0.31439829557094945, 0.2095988637139663, 0.10479943185698315, 0.34933143952327717, 0.8301041791735598, 0.774697685103767, 0.8378820009006621, 0.5297547726752043, 0.13243869316880108, 0.19865803975320162, 0.06621934658440054, 0.16068106198959617, 0.26780176998266025, 0.26780176998266025, 0.16068106198959617, 0.053560353996532056, 0.12213435390319136, 0.12213435390319136, 0.7328061234191482, 0.12213435390319136, 0.5110975379069044, 0.7325536373499931, 0.9489557847405892, 0.9569166970186063, 0.7276838932209474, 0.24256129774031582, 0.2499675736833238, 0.2499675736833238, 0.4999351473666476, 0.773398885642629, 0.7230518577015619, 0.8941087712023223, 0.79910545637399, 0.22831584467828286, 0.9320807457315778, 0.9647170988059665, 0.13733390560423825, 0.22888984267373044, 0.2746678112084765, 0.18311187413898433, 0.18311187413898433, 0.8173009422617519, 0.16346018845235039, 0.258128648195107, 0.258128648195107, 0.34417153092680935, 0.17208576546340468, 0.31635903486633943, 0.30370467347168584, 0.12654361394653577, 0.10123489115722861, 0.15185233673584292, 0.9569250227612397, 0.9566436886435564, 0.9333847712951554, 0.9172119469916605, 0.871046748094646, 0.8379370821014068, 0.35719170017383595, 0.11906390005794532, 0.47625560023178126, 0.7241604183201347, 0.36237506820130505, 0.18118753410065253, 0.5435626023019575, 0.8062896972078742, 0.7244337427648825, 0.07265375344453677, 0.8718450413344413, 0.5163490678693522, 0.22129245765829378, 0.1475283051055292, 0.0737641525527646, 0.0737641525527646, 0.8061947578796158, 0.8780132086418867, 0.7574425957525238, 0.15148851915050476, 0.3158958612892989, 0.6317917225785978, 0.5022927560228281, 0.25114637801141404, 0.18835978350856053, 0.06278659450285351, 0.14055924807902603, 0.5622369923161041, 0.09370616538601736, 0.09370616538601736, 0.04685308269300868, 0.1797609210658045, 0.1797609210658045, 0.359521842131609, 0.08988046053290225, 0.2696413815987067, 0.3911122878001135, 0.19555614390005674, 0.3911122878001135, 0.9499945049864098, 0.8312561241946437, 0.9768766286700895, 0.5111417976074637, 0.9568567382630395, 0.7244364311108671, 0.9797028332393274, 0.19183458452520888, 0.25577944603361186, 0.25577944603361186, 0.06394486150840296, 0.19183458452520888, 0.72430023984057, 0.894434591703681, 0.7749897527336714, 0.13245645476754925, 0.7947387286052954, 0.8780269660629509, 0.3144429062076854, 0.33191195655255684, 0.08734525172435706, 0.1397524027589713, 0.1397524027589713, 0.737460186820775, 0.24582006227359166, 0.25121512368634724, 0.7536453710590417, 0.2931971569378474, 0.1465985784689237, 0.43979573540677114, 0.07329928923446186, 0.889891606075675, 0.2756002839074044, 0.18373352260493625, 0.3674670452098725, 0.09186676130246813, 0.09186676130246813, 0.19109367170807443, 0.45862481209937866, 0.11465620302484467, 0.07643746868322977, 0.15287493736645955, 0.23459697849620345, 0.7037909354886104, 0.36805626328939506, 0.6134271054823252, 0.5111497320797761, 0.8328964372318723, 0.7673066122906883, 0.21923046065448237, 0.5947279441152796, 0.16406288113524956, 0.10253930070953098, 0.04101572028381239, 0.10253930070953098, 0.20939124812348892, 0.20939124812348892, 0.6281737443704667, 0.3521351902415993, 0.3873487092657592, 0.17606759512079964, 0.03521351902415993, 0.07042703804831986, 0.9500435705949758, 0.7748365612405632, 0.8780158422079372, 0.3406662215487301, 0.17033311077436505, 0.3406662215487301, 0.08516655538718253, 0.9685797694163499, 0.8361987410477624, 0.8897079340300466, 0.21278990006657977, 0.21278990006657977, 0.6383697001997393, 0.8943074034195326, 0.9647367275815737, 0.9320833164518123, 0.8898042221316713, 0.48818199314315186, 0.48818199314315186, 0.3506276585820064, 0.0500896655117152, 0.1001793310234304, 0.3005379930702912, 0.250448327558576, 0.7244416243487717], \"Term\": [\"AD\", \"ADSCs\", \"AG\", \"AID\", \"AIDBBS\", \"Automated\", \"Benefits\", \"Burdens\", \"C\", \"CCI\", \"CCN\", \"CGM\", \"CI\", \"CI\", \"COMT\", \"COVID\", \"COVID\", \"COVID\", \"DME\", \"DNA\", \"DReSS\", \"Delivery\", \"ES\", \"ETS\", \"FMS\", \"Fournier\", \"GCDCO\", \"GDM\", \"GHNF\", \"Group\", \"HFpEF\", \"HGinduced\", \"HIIT\", \"HRF\", \"HS\", \"HSK\", \"HTRB\", \"Heart\", \"Intraretinal\", \"KP\", \"Limb\", \"MARD\", \"MASLD\", \"MICT\", \"MTMAA\", \"MetS\", \"MitoEsc\", \"N\", \"NTX\", \"OPRM\", \"OVX\", \"P\", \"P\", \"P\", \"P\", \"PBMAs\", \"PCOS\", \"PCSK\", \"PCa\", \"PE\", \"PESI\", \"PJI\", \"PMSGH\", \"Pg\", \"QT\", \"RAIA\", \"SGLT\", \"Silicone\", \"TB\", \"VICs\", \"W\", \"accuracy\", \"acid\", \"acid\", \"acid\", \"acid\", \"activity\", \"activity\", \"activity\", \"activity\", \"adult\", \"adult\", \"adult\", \"adult\", \"adverse\", \"adverse\", \"affect\", \"affect\", \"affect\", \"affect\", \"also\", \"also\", \"also\", \"also\", \"also\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"ankle\", \"antiTB\", \"antibacterial\", \"antibacterial\", \"apheresis\", \"approach\", \"approach\", \"approach\", \"asprosin\", \"atherosclerosis\", \"bioreactor\", \"brain\", \"bread\", \"calcification\", \"calcification\", \"cancer\", \"cancer\", \"cancer\", \"cardiac\", \"cardiac\", \"cardiac\", \"cardiovascular\", \"cardiovascular\", \"cardiovascular\", \"cardiovascular\", \"care\", \"care\", \"cascade\", \"cascade\", \"cell\", \"cell\", \"cell\", \"cell\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"coagulation\", \"concentration\", \"concentration\", \"concentration\", \"conductive\", \"control\", \"control\", \"control\", \"control\", \"conventional\", \"corneal\", \"corrected\", \"crosssectional\", \"day\", \"day\", \"decreased\", \"detraining\", \"diabetes\", \"diabetes\", \"diabetes\", \"diabetes\", \"diabetes\", \"diabetic\", \"diabetic\", \"diabetic\", \"diabetic\", \"diabetic\", \"differentiated\", \"differentiated\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"distant\", \"dos\", \"drug\", \"drug\", \"drug\", \"drug\", \"drug\", \"duration\", \"dysfunction\", \"dysfunction\", \"dysfunction\", \"dyslipidemia\", \"dyslipidemia\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"ejection\", \"electrolyte\", \"event\", \"event\", \"event\", \"event\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"foot\", \"foot\", \"fraction\", \"gangrene\", \"gene\", \"gene\", \"germ\", \"gland\", \"glioblastoma\", \"glucose\", \"glucose\", \"glucose\", \"glucose\", \"gouty\", \"group\", \"group\", \"group\", \"group\", \"healthcare\", \"healthcare\", \"higher\", \"higher\", \"higher\", \"higher\", \"hospitalized\", \"hospitalized\", \"hospitalized\", \"identify\", \"identify\", \"identify\", \"incidence\", \"including\", \"including\", \"including\", \"including\", \"including\", \"induced\", \"induced\", \"induced\", \"induced\", \"induced\", \"inflammation\", \"inflammation\", \"inflammation\", \"infusion\", \"infusion\", \"inhibitor\", \"inhibitor\", \"inhibitor\", \"inhibitor\", \"insulin\", \"insulin\", \"insulin\", \"islet\", \"isolates\", \"kidney\", \"kidney\", \"kidney\", \"kidney\", \"lacrimal\", \"level\", \"level\", \"level\", \"level\", \"lncRNAs\", \"lung\", \"magnesium\", \"management\", \"management\", \"management\", \"management\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"mean\", \"measure\", \"meat\", \"metastasis\", \"millet\", \"mitochondrial\", \"mitochondrial\", \"mortality\", \"mortality\", \"mortality\", \"nanoagents\", \"neutrophil\", \"nomogram\", \"number\", \"number\", \"nursing\", \"observation\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"pain\", \"pain\", \"pathway\", \"pathway\", \"pathway\", \"pathway\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"pearl\", \"periodontitis\", \"photodynamic\", \"pneumonia\", \"polymorphism\", \"potassium\", \"practice\", \"practice\", \"practice\", \"preserved\", \"property\", \"property\", \"property\", \"prostate\", \"psychophysical\", \"r\", \"r\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"ratio\", \"readmission\", \"receiving\", \"receiving\", \"retinal\", \"retinal\", \"review\", \"review\", \"review\", \"review\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"role\", \"role\", \"role\", \"role\", \"role\", \"score\", \"score\", \"score\", \"selfcare\", \"semaglutide\", \"senescence\", \"senescent\", \"sensor\", \"sensoryrelated\", \"serum\", \"significant\", \"significant\", \"significant\", \"significant\", \"significant\", \"sleep\", \"starch\", \"stay\", \"stem\", \"stem\", \"stone\", \"study\", \"study\", \"study\", \"study\", \"study\", \"surgical\", \"surgical\", \"survival\", \"survival\", \"system\", \"system\", \"system\", \"system\", \"tear\", \"therapeutic\", \"therapeutic\", \"therapeutic\", \"therapeutic\", \"therapeutic\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"therapy\", \"three\", \"three\", \"thyroid\", \"thyroid\", \"tophus\", \"transplant\", \"transplantation\", \"transplantation\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tumor\", \"tumor\", \"tumor\", \"type\", \"type\", \"type\", \"type\", \"type\", \"ulcer\", \"understudied\", \"uric\", \"using\", \"using\", \"using\", \"using\", \"vaccination\", \"valve\", \"varicose\", \"vascular\", \"vascular\", \"vascular\", \"vasopressor\", \"vein\", \"venous\", \"visual\", \"woman\", \"woman\", \"wound\", \"wound\", \"wound\", \"wound\", \"wound\", \"x\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 4, 5, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el14781375744792522721984962071\", ldavis_el14781375744792522721984962071_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el14781375744792522721984962071\", ldavis_el14781375744792522721984962071_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el14781375744792522721984962071\", ldavis_el14781375744792522721984962071_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the number of documents per topic\n",
        "topic_counts = np.bincount(dominant_topics)\n",
        "\n",
        "# Plot the distribution of topics with generic names\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, n_topics + 1), topic_counts, tick_label=[f'Topic {i + 1}' for i in range(n_topics)])\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Distribution of Topics')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "l8qwcyWDyLTl",
        "outputId": "434ae4d0-9014-4919-848e-06bd66cb1fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function to create feature vectors\n",
        "def create_feature_vector(lda_corpus, num_topics):\n",
        "    feature_vectors = []\n",
        "    for doc in lda_corpus:\n",
        "        vector = np.zeros(num_topics)\n",
        "        if isinstance(doc, list):\n",
        "            for topic_num, topic_prob in doc:\n",
        "                vector[topic_num] = topic_prob\n",
        "        feature_vectors.append(vector)\n",
        "    return np.array(feature_vectors)\n",
        "\n",
        "# Number of topics (should match the number of topics in your LDA model)\n",
        "num_topics = 5\n",
        "\n",
        "# Create feature vectors\n",
        "X_topics = create_feature_vector(lda_corpus, num_topics)\n",
        "\n",
        "# Print the shape of the feature matrix\n",
        "print(f\"Feature matrix shape: {X_topics.shape}\")\n",
        "\n",
        "# Ensure the labels match the number of samples in X_topics\n",
        "num_samples = X_topics.shape[0]\n",
        "\n",
        "# Example labels (replace with actual labels if available)\n",
        "# Create labels to match the number of samples in X_topics\n",
        "labels = ['Label1', 'Label2', 'Label3'] * ((num_samples // 3) + 1)\n",
        "labels = labels[:num_samples]  # Ensure labels list matches the length of the data\n",
        "\n",
        "# Verify lengths before encoding\n",
        "print(f\"Number of samples in X_topics: {len(X_topics)}\")\n",
        "print(f\"Number of labels: {len(labels)}\")\n",
        "print(f\"X_topics: {X_topics[:5]}\")  # First 5 samples\n",
        "print(f\"Labels: {labels[:5]}\")      # First 5 labels\n",
        "\n",
        "# Ensure consistent lengths before encoding\n",
        "assert len(X_topics) == len(labels), \"Mismatch between the number of samples and labels.\"\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Verify lengths again after encoding\n",
        "print(f\"Number of encoded labels: {len(y)}\")\n",
        "print(f\"Encoded Labels: {y[:5]}\")  # First 5 encoded labels\n",
        "\n",
        "# Ensure consistent lengths after encoding\n",
        "assert len(X_topics) == len(y), \"Mismatch between the number of samples and encoded labels.\"\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_topics, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the splits\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg_model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Print model training completion\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Logistic Regression Performance:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edcJ5qWczYte",
        "outputId": "1f42518c-72eb-4b23-ef2e-3cd7fbf04a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shape: (100, 5)\n",
            "Number of samples in X_topics: 100\n",
            "Number of labels: 100\n",
            "X_topics: [[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "Labels: ['Label1', 'Label2', 'Label3', 'Label1', 'Label2']\n",
            "Number of encoded labels: 100\n",
            "Encoded Labels: [0 1 2 0 1]\n",
            "X_train shape: (80, 5)\n",
            "X_test shape: (20, 5)\n",
            "y_train shape: (80,)\n",
            "y_test shape: (20,)\n",
            "Model training completed.\n",
            "Logistic Regression Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Label1       0.00      0.00      0.00         8\n",
            "      Label2       0.00      0.00      0.00         7\n",
            "      Label3       0.25      1.00      0.40         5\n",
            "\n",
            "    accuracy                           0.25        20\n",
            "   macro avg       0.08      0.33      0.13        20\n",
            "weighted avg       0.06      0.25      0.10        20\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "\n",
        "# Print the predicted labels\n",
        "print(\"Predicted labels:\")\n",
        "print(y_pred)\n",
        "\n",
        "# Evaluate the model performance\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Print accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0VvZhhM4s-p",
        "outputId": "26285b91-8bc2-4b97-a8fa-e21e91a1be5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels:\n",
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Label1       0.00      0.00      0.00         8\n",
            "      Label2       0.00      0.00      0.00         7\n",
            "      Label3       0.25      1.00      0.40         5\n",
            "\n",
            "    accuracy                           0.25        20\n",
            "   macro avg       0.08      0.33      0.13        20\n",
            "weighted avg       0.06      0.25      0.10        20\n",
            "\n",
            "Confusion Matrix:\n",
            "[[0 0 8]\n",
            " [0 0 7]\n",
            " [0 0 5]]\n",
            "Accuracy: 0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}